\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Primes}{\mathbb{P}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\txtand}{\text{ and }}
\newcommand{\txtor}{\text{ or }}
\newcommand{\lxor}{\veebar}

%opening
\title{Notes on Calculus}
\author{Paul Dubois}
\date{}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		
	\end{abstract}
	
	
	\section{Linear Independence Property}
	
	Given a set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ in a vector space $V$, the set is said to be \textbf{linearly independent} if the only solution to the equation
	$$
	c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \dots + c_n\mathbf{v}_n = \mathbf{0}
	$$
	is 
	$$
	c_1 = c_2 = \dots = c_n = 0.
	$$\\
	In other words, none of the vectors in the set can be written as a linear combination of the others.
	
	\noindent \textbf{Example:}
	Consider the vectors
	$\mathbf{v}_1 = \begin{pmatrix} 3 \\ 2 \\ 4 \\ -1 \\ 2 \end{pmatrix}$,
	$\mathbf{v}_2 = \begin{pmatrix} -2 \\ 5 \\ 0 \\ 1 \\ 1 \end{pmatrix}$
	and
	$\mathbf{v}_3 = \begin{pmatrix} 1 \\ 0 \\ 5 \\ 3 \\ -4 \end{pmatrix}$
	in $\mathbb{R}^5$.
	These vectors are linearly independent because the only solution to
	$$
	c_1v_1 + c_2v_2 + c_3v_3 = \mathbf{0}
	$$
	is $c_1 = c_2 = c_3 = 0$.
	
	\section{Spanning Property}
	
	A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ in a vector space $V$ is said to \textbf{span} $V$ if every vector in $V$ can be expressed as a linear combination of these vectors.
	Mathematically, the set spans $V$ if for every vector $\mathbf{v} \in V$, there exist scalars $c_1, c_2, \dots, c_n$ such that
	$$
	\mathbf{v} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \dots + c_n\mathbf{v}_n.
	$$
	
	\noindent \textbf{Example:}
	In $\mathbb{R}^2$, the vectors
	$\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$,
	$\mathbf{v}_2 = \begin{pmatrix} -1 \\ 2 \end{pmatrix}$
	and
	$\mathbf{v}_3 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$
	span $\mathbb{R}^2$ because any vector
	$\mathbf{v} = \begin{pmatrix} x \\ y \end{pmatrix}$
	can be written as
	$$
	\mathbf{v} = x\mathbf{v}_1 + (y-x)\mathbf{v}_2 + (y-x)\mathbf{v}_2.
	$$
	
	\section{Basis}
	
	A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ in a vector space $V$ is called a \textbf{basis} for $V$ if:
	\begin{itemize}
		\item The set is linearly independent.
		\item The set spans the vector space $V$.
	\end{itemize}
	
	The number of vectors in a basis is called the \textbf{dimension} of the vector space.
	
	\noindent \textbf{Example:}
	The vectors $\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ and $\mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$ form a basis for $\mathbb{R}^2$.
	The dimension of $\mathbb{R}^2$ is 2.
	
	\section{(Reduced) Row Echelon Form}
	
	A matrix is in \textbf{row echelon form} if it satisfies the following conditions:
	\begin{enumerate}
		\item All non-zero rows are above any rows of all zeros.
		\item The leading entry of each non-zero row after the first occurs to the right of the leading entry of the previous row.
		\item The leading entry in any non-zero row is 1 (this condition is for the \textbf{reduced} row echelon form).
		\item The leading 1 is the only non-zero entry in its column (for \textbf{reduced} row echelon form).
	\end{enumerate}
	
	\textbf{Example:} The matrix
	$$
	\begin{pmatrix}
		1 & 2 & 3 \\
		0 & 1 & 4 \\
		0 & 0 & 1
	\end{pmatrix}
	$$
	is in row echelon form.
	
	\section{Inverting a Matrix}
	
	The \textbf{inverse} of a square matrix $A$ is denoted by $A^{-1}$ and is defined as the matrix that satisfies
	$$
	AA^{-1} = A^{-1}A = I,
	$$
	where $I$ is the identity matrix.
	
	\textbf{Steps to find the inverse of a matrix:}
	\begin{enumerate}
		\item Form the augmented matrix $[A | I]$.
		\item Use row operations to convert $A$ into the identity matrix.
		\item The matrix that results from the identity matrix on the left side is $A^{-1}$ on the right side.
	\end{enumerate}
	
	\textbf{Example:} For the matrix
	$$
	A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix},
	$$
	the augmented matrix is
	$$
	[A | I] = \begin{pmatrix}
		1 & 2 & \mid & 1 & 0\\
		3 & 4 & \mid & 0 & 1
	\end{pmatrix}
	$$
	after reducing to row echelon form:
	$$
	[I | A^{-1}] = \begin{pmatrix}
		1 & 0 & \mid & -2 & 1\\
		0 & 1 & \mid & 1.5 & -0.5
	\end{pmatrix}
	$$
	hence, the inverse is
	$$
	A^{-1} = \begin{pmatrix}
		-2 & 1\\
		1.5 & -0.5
	\end{pmatrix}
	$$
	
	\section{Determinant}
	
	The \textbf{determinant} of a square matrix $A$, denoted by $\text{det}(A)$, is a scalar value that can be computed from the elements of the matrix. The determinant has important properties and applications, including:
	\begin{itemize}
		\item A matrix is invertible if and only if its determinant is non-zero.
		\item The determinant of a product of matrices is the product of their determinants: $\text{det}(AB) = \text{det}(A)\text{det}(B)$.
	\end{itemize}
	
	For a $2 \times 2$ matrix $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, the determinant is calculated as:
	$$
	\text{det}(A) = ad - bc.
	$$
	
	For a $3 \times 3$ matrix $A = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} $, the determinant is calculated as:
	$$
	\text{det}(A) =
	  a \begin{vmatrix} e & f \\ h & i \end{vmatrix}
	- b \begin{vmatrix} d & f \\ g & i \end{vmatrix}
	+ c \begin{vmatrix} d & e \\ g & h \end{vmatrix}.
	$$
	
	Expanding the $2 \times 2$ determinants (minors):
	$$
	\text{det}(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
	$$
	
	For higher dimension matrices, ask a computer (it's outside the scope of this course).
	
	\noindent \textbf{Example:} For the matrix
	$$
	B = \begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix},
	$$
	$\det{B} = 0$, hence, the matrix is not invertible (you can try to invert it, you will encounter a problem).
	
	While for the matrix
	$$
	A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix},
	$$
	$\det{A} = -2 \neq 0$, hence, the matrix is invertible (and we found the inverse in the previous section).
	
	% linear independence property
	% spanning property
	% basis (def)
	% row echelon form
	% inverting a matrix
	% determinent
	
\end{document}
