\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd}
\usepackage{hyperref}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Primes}{\mathbb{P}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\txtand}{\text{ and }}
\newcommand{\txtor}{\text{ or }}
\newcommand{\lxor}{\veebar}

%opening
\title{Notes on Principal Components Analysis}
\author{DSBA Mathematics Refresher 2024}
\date{}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		
	\end{abstract}
	
	\section{Sample vs. Population}
	In statistics, it is crucial to distinguish between a \textbf{sample} and a \textbf{population}:
	\begin{itemize}
		\item \textbf{Population:}
		The entire set of individuals or observations that we are interested in studying. For example, all people in a country.
		\item \textbf{Sample:}
		A subset of the population that is used to represent the entire population. For instance, 1,000 people surveyed from the population.
	\end{itemize}
	
	We often cannot measure the entire population due to time, cost, or logistical constraints.
	We rely on samples to make inferences about the population.
	
	\section{Mean, Standard Deviation, and Estimators}
	\subsection{Population Mean and Standard Deviation}
	Given a population with $N$ elements, the \textbf{population mean} $\mu$ and the \textbf{population standard deviation} $\sigma$ are defined as:
	$$
	\mu = \frac{1}{N} \sum_{i=1}^N x_i
	$$
	$$
	\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2}
	$$
	
	\subsection{Sample Mean and Standard Deviation}
	For a sample of size $n$, the \textbf{sample mean} $\bar{x}$ and the \textbf{sample standard deviation} $s$ are calculated as:
	$$
	\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
	$$
	$$
	s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2}
	$$
	
	\subsection{Why is the Variance Estimator Biased?}
	The sample variance estimator is given by:
	$$
	s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
	$$
	This estimator is biased because it tends to underestimate the true population variance $\sigma^2$.
	The bias arises because $\bar{x}$ is itself a random variable that depends on the sample, and it pulls the variance down slightly.
	This correction by dividing by $(n-1)$ instead of $n$ is known as Bessel's correction \footnote{See \url{https://gregorygundersen.com/blog/2019/01/11/bessel/} for more in depth explanation.}.
	
	\subsection{When to Use $\frac{1}{n}$ vs. $\frac{1}{n-1}$}
	\paragraph{Population Data}
	When you have data for the entire population, use:
	$$
	\sigma^2 = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2
	$$
	Here, you divide by $N$, the total number of observations in the population.
	
	\paragraph{Sample Data}
	When you have data for a sample and wish to estimate the population variance, use:
	$$
	s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
	$$
	This adjustment accounts for the extra variability introduced by using the sample mean $\bar{x}$ instead of the population mean $\mu$.
	
	\section{Change of Basis}
	In linear algebra, a \textbf{change of basis} refers to expressing a vector in a different coordinate system.
	Suppose $\mathbf{v}$ is a vector in a vector space with basis $\mathbf{B} = \{\mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_n\}$.
	If we have a new basis $\mathbf{B}' = \{\mathbf{b}_1', \mathbf{b}_2', \dots, \mathbf{b}_n'\}$, we can represent $\mathbf{v}$ in the new basis by finding the coordinates relative to $\mathbf{B}'$.
	
	If $\mathbf{v} = a_1 \mathbf{b}_1 + a_2 \mathbf{b}_2 + \dots + a_n \mathbf{b}_n$, then under the new basis $\mathbf{B}'$, the same vector can be written as:
	$$
	\mathbf{v} = a_1' \mathbf{b}_1' + a_2' \mathbf{b}_2' + \dots + a_n' \mathbf{b}_n'
	$$
	The coordinates $\mathbf{a'} = (a_1', a_2', \dots, a_n')$ are related to the original coordinates $\mathbf{a} = (a_1, a_2, \dots, a_n)$ by a transformation matrix $\mathbf{P}$:
	$$
	\mathbf{a} = \mathbf{P} \mathbf{a'}
	\qquad \textit{or} \qquad
	\mathbf{a'} = \mathbf{P}^{-1} \mathbf{a}
	$$
	where $P = \begin{pmatrix} \mathbf{b}_1' \mathbf{b}_2' \dots \mathbf{b}_n' \end{pmatrix}$ with $\mathbf{b}_1', \mathbf{b}_2', \dots, \mathbf{b}_n'$ column vectors expressed in the basis $\mathbf{B}$.
	
	\paragraph{Example: Change of Basis}
	\noindent\\
	Consider a vector $\mathbf{v} = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$ in the standard basis $\mathbf{B} = \left\{ \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix} \right\}$.
	\\
	Suppose we want to express $\mathbf{v}$ in a new basis $\mathbf{B}' = \left\{ \mathbf{b}_1' = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \mathbf{b}_2' = \begin{pmatrix} 1 \\ -1 \end{pmatrix} \right\}$.
	
	\subparagraph{Approach 1: Solving the linear system}
	\noindent\\
	Finding the coordinates of $\mathbf{v}$ in the new basis $\mathbf{B}'$, means expressing $\mathbf{v}$ as a linear combination of $\mathbf{b}_1'$ and $\mathbf{b}_2'$:
	$$
	\mathbf{v} = c_1 \mathbf{b}_1' + c_2 \mathbf{b}_2' = c_1 \begin{pmatrix} 1 \\ 1 \end{pmatrix} + c_2 \begin{pmatrix} 1 \\ -1 \end{pmatrix}
	$$
	This gives us the system of equations:
	$$
	\begin{aligned}
		c_1 + c_2 &= 3 \\
		c_1 - c_2 &= 2
	\end{aligned}
	$$
	Solving for $c_1$ and $c_2$:
	$$
	c_1 = \frac{3 + 2}{2} = \frac{5}{2}, \quad c_2 = \frac{3 - 2}{2} = \frac{1}{2}
	$$
	So, the coordinates of $\mathbf{v}$ in the new basis $\mathbf{B}'$ are:
	$$
	\mathbf{v}_{\mathbf{B}'} = \begin{pmatrix} \frac{5}{2} \\ \frac{1}{2} \end{pmatrix}
	$$
	
	\subparagraph{Approach 2: Using the transformation matrix}
	\noindent\\
	The transformation matrix $\mathbf{P}$ from the standard basis $\mathbf{B}$ to the new basis $\mathbf{B}'$ is given by:
	$$
	\mathbf{P} = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
	$$
	and the inverse of $\mathbf{P}$ is:
	$$
	\mathbf{P}^{-1} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
	$$
	Thus, the coordinates in the new basis can also be computed as:
	$$
	\mathbf{v}_{\mathbf{B}'} = \mathbf{P}^{-1} \mathbf{v} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} 3 \\ 2 \end{pmatrix} = \begin{pmatrix} \frac{5}{2} \\ \frac{1}{2} \end{pmatrix}
	$$
	
	\section{Principal Component Analysis (PCA) Theory}
	\textbf{Principal Component Analysis (PCA)} is a technique used to reduce the dimensionality of data while preserving as much variance as possible.
	It does so by finding a new basis in which the first few dimensions capture the most variance in the data.
	All but the first few components can be discarded without loosing too much information, while making the data (much) easier to process.
	
	\paragraph{Steps to perform PCA}
	\begin{enumerate}
		\item \textbf{Standardize the Data:}
		Subtract the mean and divide by the standard deviation for each feature.
		\item \textbf{Compute the Covariance Matrix:}
		For a dataset with $n$ features, compute the $n \times n$ covariance matrix.
		\item \textbf{Eigenvalue Decomposition:}
		Perform an eigenvalue decomposition of the covariance matrix to find the eigenvalues and eigenvectors.
		\item \textbf{Select Principal Components:}
		The eigenvectors corresponding to the largest eigenvalues are chosen as the principal components.
		\item \textbf{Transform the Data:}
		Project the original data onto the principal components to obtain the transformed data in the new basis.
	\end{enumerate}
	
	\paragraph{Mathematical Formulation}
	Given a data matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$ where $m$ is the number of samples and $n$ is the number of features, the covariance matrix $\mathbf{C}$ is:
	$$
	\mathbf{C} = \frac{1}{m-1} \mathbf{X}^\top \mathbf{X}
	$$
	The eigenvalue decomposition of $\mathbf{C}$ gives:
	$$
	\mathbf{C} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^\top
	$$
	where $\mathbf{V}$ is the matrix of eigenvectors and $\mathbf{\Lambda}$ is the diagonal matrix of eigenvalues.
	The principal components are the columns of $\mathbf{V}$ corresponding to the largest eigenvalues.
	
	
	% sample vs population
	% mean / std & their etimators
	% why is variance estimator biased?
	% when to use /n vs /(n-1)
	
	% change of basis
	% PCA theory
	
\end{document}
